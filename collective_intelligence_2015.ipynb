{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "#Notebook for Colletive Intelligence 2015\n",
    "##Contact: NicolÃ¡s Della Penna me@nikete.com, Dhaval Adjodah, dhaval@mit.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.load_extensions('usability/codefolding/main');\n",
       "// code folding capability as per: https://github.com/ipython-contrib/IPython-notebook-extensions/wiki/Codefolding"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.load_extensions('usability/codefolding/main');\n",
    "// code folding capability as per: https://github.com/ipython-contrib/IPython-notebook-extensions/wiki/Codefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [],
   "source": [
    "%%script R --no-save\n",
    "## R script to download data and prepare features\n",
    "# dhaval@mit.edu\n",
    "# this script will download all data from the SciCast datamart, compute the necessary features and output it as a CSV. \n",
    "# This takes about 10min on an i7 CPU with 8gb RAM\n",
    "# note: you will need to request your API_KEY_FROM_SCICAST as per https://scicast.files.wordpress.com/2014/10/scicast_datamart_guide_v1-21.pdf\n",
    "\n",
    "{#library\n",
    "  library('lubridate')\n",
    "  library('sqldf')\n",
    "  library('reshape')\n",
    "  library('stringr')\n",
    "  library('RJSONIO')\n",
    "}# libraries\n",
    "\n",
    "\n",
    "{ # downloading and importing data\n",
    "  # load(\".RData\")\n",
    "  # \n",
    "#   link_question_summary = 'http://datamart.scicast.org/question/?format=csv&api_key=API_KEY_FROM_SCICAST'\n",
    "#   download.file(url=link_question_summary, destfile='question_summary.csv', method='wget')\n",
    "  question_summary <- read.csv(\"question_summary.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "#   \n",
    "#   \n",
    "#   link_question_history = 'http://datamart.scicast.org/question_history/?format=csv&api_key=API_KEY_FROM_SCICAST'\n",
    "#   download.file(url=link_question_history, destfile='question_history.csv', method='wget')\n",
    "  question_history <- read.csv(\"question_history.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "#   \n",
    "#   \n",
    "#   link_person = 'http://datamart.scicast.org/person/?format=csv&api_key=API_KEY_FROM_SCICAST'\n",
    "#   download.file(url=link_person, destfile='person.csv', method='wget')\n",
    "  person <- read.csv(\"person.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "#   \n",
    "#   link_trade_history = 'http://datamart.scicast.org/trade_history/?format=csv&api_key=API_KEY_FROM_SCICAST'\n",
    "#   download.file(url=link_trade_history, destfile='trade_history.csv', method='wget')\n",
    "  trade_history <- read.csv(\"trade_history.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "#   \n",
    "#   \n",
    "#   link_comment = 'http://datamart.scicast.org/comment/?format=csv&api_key=API_KEY_FROM_SCICAST'\n",
    "#   download.file(url=link_comment, destfile='comment.csv', method='wget')\n",
    "  comment <- read.csv(\"comment.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "#   \n",
    "#   link_leaderboard = 'http://scicast.org:8200/person/leaderboard/?format=csv&api_key=API_KEY_FROM_SCICAST&start_date=05-26-2015'\n",
    "#   download.file(url=link_leaderboard, destfile='leaderboard.csv', method='wget')\n",
    "  leaderboard <- read.csv(\"leaderboard.csv\", na.strings=\"None\", stringsAsFactors=FALSE)\n",
    "  # \n",
    "  # rm(link_comment, link_leaderboard, link_person, link_question_history, link_question_summary, link_trade_history)\n",
    "}# downloading and importing data\n",
    "\n",
    "{# date conversion, getting max dates\n",
    "  # \n",
    "  date_question_history = trade_history\n",
    "  # date_question_history$traded_at = as.Date(substr(trade_history$traded_at, 0, 10), \"%Y-%m-%d\")\n",
    "  \n",
    "  # date_question_history$traded_at = as.POSIXlt(trade_history$traded_at)\n",
    "  date_question_history$traded_at = as.numeric(as.POSIXct(trade_history$traded_at))\n",
    "  \n",
    "  #getting last trade information:\n",
    "  last_trade = sqldf(\"select                     \n",
    "                     question_id,\n",
    "                     trade_id,\n",
    "                     choice_index,\n",
    "                     interface_type,\n",
    "                     max(traded_at) traded_at,\n",
    "                     new_value_list\n",
    "                     from date_question_history\n",
    "                     group by\n",
    "                     question_id                    \n",
    "                     \")\n",
    "}# date conversion, getting max dates\n",
    "\n",
    "{ # merging the datasets\n",
    "  merged_questions = merge(last_trade, question_summary, by.x = \"question_id\", by.y = \"question_id\")\n",
    "  \n",
    "  merged_questions = sqldf(\"select                     \n",
    "                          *\n",
    "                          from merged_questions\n",
    "                          where resolution_index is NOT NULL\n",
    "                          AND (classification LIKE '%binary%'\n",
    "                          OR classification LIKE '%unordered%')\n",
    "                          \")   #this is to select only multinomial questions                    \n",
    "  \n",
    "  \n",
    "  sort_leaderboard = leaderboard[order(-leaderboard$max_score),]\n",
    "  sort_leaderboard = sort_leaderboard[1:39,]\n",
    "}# merging the datasets\n",
    "\n",
    "{# getting spark data from json file\n",
    "  download.file(url='http://datamart.scicast.org/question/?format=json&api_key=API_KEY_FROM_SCICAST',\n",
    "                destfile='spark.json'\n",
    "  )\n",
    "  spark = fromJSON('spark.json')\n",
    "  \n",
    "  spark_data = data.frame()\n",
    "  spark_temp = c()\n",
    "  \n",
    "  for (i in 1:length(spark)){\n",
    "    spark_temp = c()\n",
    "    spark_question = data.frame( question_id = numeric())\n",
    "    #   print(i)\n",
    "    for (j in 1:length(spark[i][[1]]$spark_collaborators)){  \n",
    "      spark_temp = c(spark_temp, spark[i][[1]]$spark_collaborators[[j]]$predict_id)\n",
    "    }\n",
    "    spark_question[1,] = spark[i][[1]]$question_id #howto\n",
    "    spark_question$id_lists[[1]] = if(is.null(spark_temp)){NA} else{spark_temp}\n",
    "    spark_data = rbind(spark_data, spark_question)\n",
    "  }\n",
    "  rm(i, j, spark_question, spark_temp, spark)\n",
    "}# getting spark data from json file\n",
    "\n",
    "{#looping over each question\n",
    "  numtrades = 400# 50\n",
    "  final = data.frame()\n",
    "  t = now()\n",
    "  \n",
    "  num_questions = nrow(merged_questions)\n",
    "  \n",
    "  for(i in 1:num_questions) {\n",
    "    id = merged_questions$question_id[i]\n",
    "    cat(paste(num_questions-i, \" \"))  \n",
    "{# preparing the data: sql_history, prize_question, half-time, etc\n",
    "      all_choice_values = data.frame()\n",
    "      question_trade_volume = 0\n",
    "      \n",
    "      \n",
    "      question_sub = subset(merged_questions, merged_questions$question_id == id)\n",
    "      \n",
    "      sql_trade_history =  fn$sqldf(\" select                     \n",
    "                                    question_id,\n",
    "                                    choice_index,\n",
    "                                    traded_at,\n",
    "                                    interface_type,\n",
    "                                    new_value_list,\n",
    "                                    old_value_list,\n",
    "                                    user_id,\n",
    "                                    assets_per_option\n",
    "                                    from date_question_history\n",
    "                                    where question_id = $id\n",
    "                                    order by traded_at\n",
    "                                    limit $numtrades\n",
    "                                    \")\n",
    "      \n",
    "      sql_uncertainty =  fn$sqldf(\" select                     \n",
    "                                  question_id,\n",
    "                                  count(*) count,                             \n",
    "                                  max(traded_at) last_trade\n",
    "                                  from date_question_history\n",
    "                                  where question_id = $id\n",
    "                                  group by question_id\n",
    "                                  \")\n",
    "      \n",
    "      lastrowcount = floor(0.9*sql_uncertainty$count)\n",
    "      #print(paste(\"last row=\",lastrowcount))\n",
    "      sql_last_row = fn$sqldf(\"select                     \n",
    "                              question_id,\n",
    "                              new_value_list,\n",
    "                              traded_at\n",
    "                              from date_question_history\n",
    "                              where question_id = $id\n",
    "                              order by traded_at\n",
    "                              limit $lastrowcount\n",
    "                              \")\n",
    "      \n",
    "      question_num_trades = sql_uncertainty$count\n",
    "      \n",
    "      halftime = as.numeric(as.POSIXct(question_sub$created_at)) + 0.5*(sql_uncertainty$last_trade -  as.numeric(as.POSIXct(question_sub$created_at)) )\n",
    "      \n",
    "      sql_half_time = fn$sqldf(\"select                     \n",
    "                               question_id,                \n",
    "                               new_value_list,\n",
    "                               abs(traded_at - $halftime)/3600/24 AS distance_from_test\n",
    "                               from date_question_history\n",
    "                               where question_id = $id\n",
    "                               order by distance_from_test\n",
    "                               limit 1\n",
    "                               \")\n",
    "      \n",
    "      \n",
    "      prize_question = if( length(grep(\"prize\", tolower(subset(question_summary, question_summary$question_id == id)$categories))) > 0){1} else{0}\n",
    "      #   print(prize_question)\n",
    "      is_visible = 5 - nchar(subset(question_summary, question_summary$question_id == id)$is_visible)\n",
    "    }# preparing the data: sql_history, prize_question, half-time, etc\n",
    "\n",
    "\n",
    "for (c in 1:length(as.numeric(strsplit(merged_questions$new_value_list[i], ',')[[1]])) ) { \n",
    "{\n",
    "  each_choice_values = data.frame(\n",
    "    choice_happened = numeric(),\n",
    "    prob_choice_final = numeric(),\n",
    "    prize_question = numeric(),\n",
    "    is_visible= numeric(),\n",
    "    num_choices= numeric(),\n",
    "    crowd_predicted_final= numeric(),\n",
    "    crowd_predicted_90pct= numeric(),\n",
    "    choice = numeric(),\n",
    "    resolution_value_array_choice = numeric(),\n",
    "    prob_after_90pct_of_trades = numeric(),\n",
    "    prob_at_half_market = numeric(),\n",
    "    question_num_trades = numeric(),\n",
    "    question_trade_volume = numeric(),\n",
    "    asset_trade_volume = numeric()\n",
    "  )\n",
    "}# creating the empty choice values\n",
    "\n",
    "{#creating choice column names \n",
    "  trades_long_orig = data.frame(\n",
    "    \"prob_choice\" = NA,\n",
    "    \"prob_choice_change\" = NA,\n",
    "    \"assets_choice\" = NA,\n",
    "    \"power_trades_choice\" = NA,\n",
    "    \"leaderboard_trades_choice\" = NA,\n",
    "    \"spark_trade_choice\" = NA,\n",
    "    \"prob_choice_average\" = NA\n",
    "  )\n",
    "  \n",
    "  trades_long_final = data.frame(\n",
    "    \"prob_choice1\" = NA,\n",
    "    \"prob_choice_change1\" = NA,\n",
    "    \"assets_choice1\" = NA,\n",
    "    \"power_trades_choice1\" = NA,\n",
    "    \"leaderboard_trades_choice1\" = NA,\n",
    "    \"spark_trade_choice1\" = NA,\n",
    "    \"prob_choice_average1\" = NA\n",
    "  )\n",
    "  trades_long_temp = trades_long_orig\n",
    "  \n",
    "  asset_trade_volume = 0 \n",
    "  \n",
    "  for (n in c(2:4, 1:floor(numtrades/5)*5) ){\n",
    "    \n",
    "    names(trades_long_temp) <- paste(names(trades_long_orig), n, sep = \"\")\n",
    "    trades_long_final = cbind(trades_long_final,trades_long_temp)           \n",
    "  }\n",
    "  rm(trades_long_orig, trades_long_temp, n)\n",
    "}#creating choice column names \n",
    "\n",
    "{# vertical trade history to horizontal column names in final data\n",
    "  average_choice = c()\n",
    "  for (k in 1:numtrades){\n",
    "    #     for (k in c(1:4, 1:floor(numtrades/5)*5){\n",
    "    if (k <= dim(sql_trade_history)[1]){\n",
    "      average_choice = c(average_choice, as.numeric(strsplit(sql_trade_history$new_value_list[k], ',')[[1]])[c])\n",
    "      if (k %in% c(1:4, 1:floor(numtrades/5)*5)){\n",
    "        trades_long_final[[paste(\"prob_choice\", k, sep=\"\")]] = as.numeric(strsplit(sql_trade_history$new_value_list[k], ',')[[1]])[c]\n",
    "        trades_long_final[[paste(\"prob_choice_change\", k, sep=\"\")]] = as.numeric(strsplit(sql_trade_history$new_value_list[k], ',')[[1]])[c] - as.numeric(strsplit(sql_trade_history$old_value_list[k], ',')[[1]])[c]\n",
    "        trades_long_final[[paste(\"power_trades_choice\", k, sep=\"\")]] = sql_trade_history$interface_type[k]\n",
    "        trades_long_final[[paste(\"leaderboard_trades_choice\", k, sep=\"\")]] =  if (sql_trade_history$user_id[k] %in% sort_leaderboard$user_id) {1} else {0}\n",
    "        trades_long_final[[paste(\"assets_choice\", k, sep=\"\")]] = as.numeric(strsplit(substr(sql_trade_history$assets_per_option[k], 2, nchar(sql_trade_history$assets_per_option[k])-1), ',')[[1]])[c]\n",
    "        trades_long_final[[paste(\"spark_trade_choice\", k, sep=\"\")]] = if (sql_trade_history$user_id[k] %in% subset(spark_data, spark_data$question_id == id)$id_lists) {1} else {0}                \n",
    "        trades_long_final[[paste(\"prob_choice_average\", k, sep=\"\")]] = mean(average_choice)\n",
    "        \n",
    "        average_choice = c()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  rm(average_choice)\n",
    "}\n",
    "\n",
    "prob_choice_final = as.numeric(strsplit(question_sub$new_value_list, ',')[[1]])[c]\n",
    "choice_happened = as.numeric(question_sub$resolution_index == (c-1))\n",
    "prob_after_90pct_of_trades = if(lastrowcount>0){as.numeric(strsplit(sql_last_row$new_value_list[lastrowcount], ',')[[1]])[c]} else{NA}\n",
    "prob_at_half_market = as.numeric(strsplit(sql_half_time$new_value_list, ',')[[1]])[c]\n",
    "resolution_value_array_choice =  as.numeric(strsplit(substr(question_sub$resolution_value_array, 2, nchar(question_sub$resolution_value_array)-1), ',')[[1]])[question_sub$resolution_index+1] #this is just to say if this was a scaled winning term, question_level value\n",
    "\n",
    "for(i in 1:nrow(sql_trade_history)) {\n",
    "  asset_trade_volume = asset_trade_volume + \n",
    "    abs(\n",
    "      as.numeric(strsplit(substr(sql_trade_history$assets_per_option[i], 2, nchar(sql_trade_history$assets_per_option[i])-1), ',')[[1]])[c]\n",
    "    )\n",
    "  #       }   \n",
    "}# all assets traded for this choice\n",
    "\n",
    "\n",
    "each_choice_values[1,] = c(\n",
    "  choice_happened,\n",
    "  prob_choice_final,\n",
    "  prize_question,\n",
    "  is_visible,\n",
    "  num_choices = length(as.numeric(strsplit(question_sub$new_value_list, ',')[[1]])), #length(as.numeric(strsplit(merged_questions$new_value_list[i], ',')[[1]])),\n",
    "  crowd_predicted_final = which.max(as.numeric(strsplit(question_sub$new_value_list, ',')[[1]])) == c,\n",
    "  crowd_predicted_90pct = if(lastrowcount>0){ which.max(as.numeric(strsplit(sql_last_row$new_value_list[lastrowcount], ',')[[1]])) == c } else{NA},\n",
    "  c-1, #just a simple choice indicator in the table\n",
    "  resolution_value_array_choice,\n",
    "  prob_after_90pct_of_trades,\n",
    "  prob_at_half_market,\n",
    "  question_num_trades,\n",
    "  question_trade_volume,\n",
    "  asset_trade_volume     \n",
    ")\n",
    "\n",
    "\n",
    "all_choice_values = rbind(all_choice_values,cbind(question_sub, each_choice_values, trades_long_final))\n",
    "\n",
    "rm(trades_long_final,c, num_assets, asset_trade_volume, num_assets_question, num_trades_total_50, num_power_trades_total_50, each_choice_values,num_leaderboard_trades_total_50, num_trades_total_50)\n",
    "} #choice loop ends\n",
    "\n",
    "all_choice_values$question_trade_volume = sum(all_choice_values$asset_trade_volume)\n",
    "\n",
    "\n",
    "final = rbind(final, all_choice_values)\n",
    "\n",
    "\n",
    "rm(sql_half_time, c, lastrowcount,all_choice_values, time, x, sql_last_row, halftime, is_visible, prize_question, question_num_trades, question_trade_volume)\n",
    "\n",
    "\n",
    "  } # question loop ends\n",
    "\n",
    "\n",
    "print(Sys.time() - t)\n",
    "rm(question_sub, num_questions,sql_uncertainty, question_trade_volume, prob_after_90pct_of_trades, prob_at_half_market, id,j, t, lifespan_question, timebreak, prob, i, fraction, numtrades, choice_happened, k, leaderboard, name_choice, num_trades, prob_after_90percent_of_trades, prob_choice_final, resolution_value_array_choice) \n",
    "\n",
    "row.names(final) = NULL\n",
    "\n",
    "}# #looping over each question\n",
    "\n",
    "save.image(\".RData\")\n",
    "write.csv(final, 'prediction_market_computed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "breakpoint": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean prediction score for only last trade: 0.911669872768\n",
      "mean prediction score all but last trade : 0.903109243697\n"
     ]
    }
   ],
   "source": [
    "# me@nikete.com, dhaval@mit.edu\n",
    "# ML code for testing price information integration\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "import sklearn.preprocessing, sklearn.decomposition, sklearn.ensemble, sklearn.cross_validation, sklearn.linear_model, sklearn.pipeline, sklearn.metrics, sklearn.gaussian_process   \n",
    "from patsy import dmatrices \n",
    "\n",
    "df = pd.read_csv(\"prediction_market_computed.csv\")\n",
    "\n",
    "def feature_list(n, only_last_trade = False):\n",
    "    s = \"choice_happened ~ \"\n",
    "    if only_last_trade == True:\n",
    "        numtrades_list = [n]\n",
    "    else:\n",
    "        numtrades_list = [1,2,3,4] + map(lambda x: x*5, range(1,n/5))\n",
    "    for d in numtrades_list:\n",
    "        s +=\"+ prob_choice%d   \"%(d)\n",
    "#         s +=\"+ prob_choice%d + prob_choice_average%d \"%(d) # use this if you want to include prob_choice_average. modify to include other features \n",
    "    return s\n",
    "\n",
    "\n",
    "def model_crowd(data, model, specification, n_folds = 5):\n",
    "    y, X = dmatrices(specification, data, NA_action='drop')\n",
    "    y=y.ravel()\n",
    "    skf = sklearn.cross_validation.StratifiedKFold(y, n_folds)\n",
    "    scores=[]\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test   = X[test_index],  y[test_index]        \n",
    "        model = model.fit(X_train, y_train)\n",
    "        scores.append( model.score(X_test, y_test))        \n",
    "    return np.mean(scores)\n",
    "\n",
    "print 'mean prediction score for only last trade:',  model_crowd(df, sklearn.linear_model.LogisticRegression(), feature_list(150, True))\n",
    "print 'mean prediction score all but last trade :', model_crowd(df, sklearn.linear_model.LogisticRegression(), feature_list(150, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
